{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fancy-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of neural networks using the MNIST dataset, which is composed of 70000 28 * 28 grayscale images from 1 - 10. Here we will be\n",
    "teaching a neural network to learn how to recognize digits.\n",
    "\n",
    "This is probably the trickiest module in SeaLion to learn given how many classes \n",
    "you'll be working with - so I hope this tutorial will make it clear. \n",
    "\"\"\"\n",
    "import sealion as sl  # first import this, under sl alias\n",
    "from sealion.neural_networks.optimizers import SGD, Adam  # we'll use these 2, but feel free to try some more\n",
    "from sealion.neural_networks.loss import CrossEntropy  # this is the loss function we'll use (classification problem)\n",
    "from sealion.utils import one_hot  # one_hot function for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joined-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Load and preprocess data!\n",
    "\n",
    "from tensorflow.keras.datasets import mnist # we'll use this in the interest of time\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # unpack it\n",
    "\n",
    "X_train = X_train / 255.0 # divide by 255 for faster convergence (normalization, if you care about that)\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# one hot our labels (to make them work with the softmax)\n",
    "y_train = one_hot(y_train, depth = 10) # 10 classes\n",
    "y_test = one_hot(y_test, depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "turkish-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Build the neural network\n",
    "\n",
    "# the first part is to just build the architecture like the following :\n",
    "\n",
    "model = sl.neural_networks.models.NeuralNetwork()\n",
    "model.add(sl.neural_networks.layers.Flatten())\n",
    "model.add(sl.neural_networks.layers.Dense(784, 64, activation=sl.neural_networks.layers.LeakyReLU(leak=0.2)))\n",
    "model.add(sl.neural_networks.layers.Dense(64, 32, activation=sl.neural_networks.layers.LeakyReLU(leak=0.2)))\n",
    "model.add(sl.neural_networks.layers.Dense(32, 10, activation=sl.neural_networks.layers.Softmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "listed-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you could build it like such :\n",
    "model = sl.neural_networks.models.NeuralNetwork([\n",
    "    sl.neural_networks.layers.Flatten(),\n",
    "    sl.neural_networks.layers.Dense(784, 64, activation=sl.neural_networks.layers.LeakyReLU(leak=0.2)),\n",
    "    sl.neural_networks.layers.Dense(64, 32, activation=sl.neural_networks.layers.LeakyReLU(leak=0.2)),\n",
    "    sl.neural_networks.layers.Dense(32, 10, activation=sl.neural_networks.layers.Softmax())\n",
    "])\n",
    "\n",
    "# just a few notes. Here we created first a flatten() layer because we are taking the 28 * 28 input to just a 784 (\n",
    "# *1) vector. So our input size for the dense layer is 784. everything after that with the 64 outputs in the first\n",
    "# hidden layer, then 32 is arbitrary, and based on hyperparameter tuning. we used the leaky relu, instead of the\n",
    "# typical relu, to avoid the dying neurons problems and Softmax() activation in the end to turn this into\n",
    "# probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we'll need to finalize the model, like such :\n",
    "\n",
    "model.finalize(loss=CrossEntropy(), optimizer=SGD(lr=0.3, momentum=0.2, nesterov=True))  # \"finalize\" just\n",
    "# means set the loss function and optimizer\n",
    "\n",
    "# here we used CrossEntropy (must be used with softmax or vice versa) and the SGD optimizer. It has a learning rate\n",
    "# of 0.3, momentum of 0.2, and utilizes nesterov accelerated gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reflected-dealer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for the 1st model :  [52650]\n"
     ]
    }
   ],
   "source": [
    "# To see how complex our model is : \n",
    "num_parameters = model.num_parameters()\n",
    "print(\"Number of parameters for the 1st model : \", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dressed-washer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc : 91.84%: 100%|█████████████████████████████████████████████████| 10/10 [02:54<00:00, 17.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Training!\n",
    "\n",
    "model.train(X_train, y_train, epochs=10)  # we can then train this model for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "falling-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  -3.2098036196336994\n",
      "Regression accuracy :  0.8697891452159215\n",
      "Validation accuracy :  0.9315\n"
     ]
    }
   ],
   "source": [
    "# Step 4 : Evaluate!\n",
    "\n",
    "'''Here we will evaluate our model and see how well it did.'''\n",
    "print(\"Loss : \", model.evaluate(X_test, y_test))  # this is just the loss\n",
    "print(\"Regression accuracy : \", model.regression_evaluate(X_test, y_test))  # regression way of calculating (horrible here,\n",
    "# because this is classification)\n",
    "print(\"Validation accuracy : \", model.categorical_evaluate(X_test, y_test))  # classification accuracy, this will\n",
    "# percent the percent that was classified correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "balanced-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc : 98.29%: 100%|█████████████████████████████████████████████████| 20/20 [11:12<00:00, 33.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# It looks like we can do better. Let's build a new neural network, with a different architecture.\n",
    "\n",
    "model = sl.neural_networks.models.NeuralNetwork()\n",
    "model.add(sl.neural_networks.layers.Flatten())\n",
    "model.add(sl.neural_networks.layers.Dense(784 , 128, activation = sl.neural_networks.layers.ELU()))\n",
    "model.add(sl.neural_networks.layers.Dense(128, 64, activation = sl.neural_networks.layers.ELU()))\n",
    "model.add(sl.neural_networks.layers.Dense(64, 10, activation = sl.neural_networks.layers.Softmax()))\n",
    "\n",
    "model.finalize(loss = sl.neural_networks.loss.CrossEntropy(), optimizer = sl.neural_networks.optimizers.Adam(lr = 0.01)) #use adam\n",
    "\n",
    "model.train(X_train, y_train, epochs=20) # train this model\n",
    "\n",
    "# this will take some about 3 minutes (I have 4 cores), given the change from 64 to 128 parameters in the first layer\n",
    "# so please be patient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hidden-technology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  -1.4782596297021773\n",
      "Regression accuracy :  0.9316249609875145\n",
      "Validation accuracy :  0.968\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss : \", model.evaluate(X_test, y_test))\n",
    "print(\"Regression accuracy : \", model.regression_evaluate(X_test, y_test))\n",
    "print(\"Validation accuracy : \", model.categorical_evaluate(X_test, y_test))\n",
    "\n",
    "# It looks like we like this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "proper-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Often times training a neural network will take some time. Who wants to go through the process from the start? \n",
    "Instead, we can save our weights and biases so we can reuse them and plug them back into a neural network as we please. \n",
    "'''\n",
    "\n",
    "parameters = model.give_parameters() # get the parameters (weights + biases)\n",
    "\n",
    "# we can also just directly store this into a pickle file\n",
    "FILE_NAME = \"MNIST_weights\"\n",
    "model.pickle_params(FILE_NAME) # now the parameters will be stored in a file known as MNIST_weights.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "integral-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load in those weights using pickle\n",
    "import pickle\n",
    "with open('MNIST_weights.pickle', 'rb') as file :\n",
    "    parameters = pickle.load(file) # load back the parameters in a variable\n",
    "    \n",
    "# now we can enter in the parameters into the weights of our current architecture (so we don't have to train from scratch)\n",
    "model.enter_parameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hungarian-owner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc : 100.0%: 100%|█████████████████████████████████████████████████| 50/50 [24:26<00:00, 29.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# now let's train it some more!\n",
    "\n",
    "model.train(X_train, y_train, epochs= 50) # train this model for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "metric-stanley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  -1.0776098235212133\n",
      "Regression accuracy :  0.9480098712648173\n",
      "Validation accuracy :  0.9766\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss : \", model.evaluate(X_test, y_test))\n",
    "print(\"Regression accuracy : \", model.regression_evaluate(X_test, y_test))\n",
    "print(\"Validation accuracy : \", model.categorical_evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "otherwise-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and if you so desire, we can save the weights again.\n",
    "file_name = \"MNIST_weights2\"\n",
    "model.pickle_params(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "present-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# there's a ton of stuff we didn't go over!\n",
    "# specifically this is  : \n",
    "# the dropout layer (sl.neural_networks.Dropout) for regularization\n",
    "# other optimizers like RMSprop, Momentum, and AdaGrad\n",
    "# the other activation functions like Tanh (sl.neural_networks.layers.Tanh)\n",
    "# or Sigmoid (sl.neural_networks.layers.Sigmoid), Swish, etc.\n",
    "\n",
    "# this will generally do better and training will be much faster, so if you aren't seeing\n",
    "# any noticeable results that's because this is MNIST. Either way we hope\n",
    "# you enjoyed this tutorial and neural networks make a bit more sense\n",
    "# with SeaLion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
